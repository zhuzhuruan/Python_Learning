{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f93664b",
   "metadata": {},
   "source": [
    "# 泛化误差上界"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf7605",
   "metadata": {},
   "source": [
    "对二类分类问题，当假设空间是有限个函数的集和$F=\\{f_{1},f_{2},f_{3},\\cdots,f_{d}\\}$时，对任意一个函数$f \\in F$，至少以概率 $1-\\delta，0 \\lt \\delta \\lt 1$，以下不等式成立：\n",
    "\n",
    "$$\n",
    "R(f) \\le \\hat R(f) + \\varepsilon(d,N,\\delta)\n",
    "$$\n",
    "\n",
    "其中，\n",
    "\n",
    "$$\n",
    "\\varepsilon(d,N,\\delta) = \\sqrt {\\frac{1}{2N}(\\log d+\\log{\\frac{1}{\\delta}})}\n",
    "$$\n",
    "\n",
    "$R(f)$是期望风险，$\\hat R(f)$是经验风险，$L$是损失函数\n",
    "\n",
    "$$\n",
    "R(f) = E[L(Y, f(X))]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat R(f) = \\frac {\\sum_{i=1}^{N} E[L(y_{i}, f(x_{i}))]}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70f2d2",
   "metadata": {},
   "source": [
    "### 理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d299b662",
   "metadata": {},
   "source": [
    "泛化误差指的是模型 f 对未知数据预测的误差。事实上，**泛化误差就是期望风险 R(f)**。\n",
    "\n",
    "用一句话解释泛化误差上界，**“模型实际表现最烂能烂到什么程度”**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a312a2",
   "metadata": {},
   "source": [
    "1、定理的适用范围是**二分类问题**。这使得$L$为$0-1$损失函数，$L$的取值$\\in {0,1}$。\n",
    "\n",
    "2、集合$F$为模型的假设空间，包含了有限个备选函数。具体的个数为$d$。\n",
    "\n",
    "3、$1−\\delta$的通俗含义。 对于集合$F$中任意的函数$f$，至少以$1−\\delta$的置信度使不等式成立。**$1−\\delta$代表了这个上界的可信程度**。\n",
    "\n",
    "4、不等式含义。期望风险也就是泛化误差$R(f)$，小于等于经验风险$\\hat R(f)$ 加某个数$\\varepsilon$。经验风险$\\hat R(f) $就是模型 $f$  在训练集上的表现。假设我们训练好了一个模型 $f$  ，那么$\\hat R(f)$就是已知量了。**对不等式移项**得$R(f)-\\hat{R}(f)\\le \\varepsilon$。**根据直觉也能知道，期望风险肯定是比经验风险大的，大多少呢？可以看到，这个差距不超过$\\varepsilon$**。\n",
    "\n",
    "5、$\\varepsilon$与$R(f)$上界的关系。$\\varepsilon$是推导过程中产生的，仅为了美观。**真正影响$R(f)$上界的是$N, d,1-\\delta$这三个参数**。\n",
    "\n",
    "（1）$N$是训练样本数，$R(f)$增大，$\\varepsilon$减小，$R(f)$上界也减小，$R(f)$上界越接近$\\hat R(f)$ 。对应的解释是样本大，训练就充分，当N取极限趋于无穷时，期望风险就趋于经验风险。\n",
    "\n",
    "（2）$d$表示假设空间中备选函数的个数，$d$增大，$\\varepsilon$增大，$R(f)$上界也随之增大。这里可以理解为，可选的函数越多，模型就会变得复杂，训练更加困难，有点奥卡姆剃刀的意思。\n",
    "\n",
    "（3）置信度$1−\\delta$增大，$\\delta$减小，相应$R(f)$上界也增大。这是显然的，想要增加可信度，相应的也要放宽条件。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc6462",
   "metadata": {},
   "source": [
    "**用一句话总结定理,“在有限个备选函数的模型假设空间里，通过训练集训练出来的模型，有一定概率在测试集中的表现是靠谱的”。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba254b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
